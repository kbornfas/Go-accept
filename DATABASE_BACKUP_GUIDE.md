# Automated Database Backup Guide

## Overview
This guide provides comprehensive instructions for setting up automated PostgreSQL backups with 30-day retention across different hosting platforms.

---

## ðŸš€ Quick Setup by Platform

### Option 1: Railway (Recommended - Built-in Backups)

Railway provides **automatic daily backups** included with PostgreSQL databases.

#### Setup Steps:
1. Go to Railway Dashboard â†’ Your Project â†’ PostgreSQL service
2. Click on **"Backups"** tab
3. Automatic backups are enabled by default
4. Retention: **7 days** (built-in)

#### Extended Retention (30 Days):
Railway's built-in backups are limited to 7 days. For 30-day retention, set up manual backups:

```bash
# Add to your Railway project settings
# Settings â†’ Deployments â†’ Add Cron Job

# Backup script (runs daily at 2 AM UTC)
0 2 * * * pg_dump $DATABASE_URL | gzip > /app/backups/backup_$(date +\%Y\%m\%d).sql.gz
```

#### Restore from Backup:
```bash
# From Railway CLI
railway run psql $DATABASE_URL < backup.sql

# Or via Railway dashboard:
# 1. Go to Backups tab
# 2. Click "Restore" on desired backup
# 3. Confirm restoration
```

**Cost:** Free (included with PostgreSQL plan)

---

### Option 2: Render

Render provides **automated daily backups** for PostgreSQL databases.

#### Setup Steps:
1. Dashboard â†’ Your PostgreSQL Database
2. Navigate to **"Backups"** section
3. Backups are automatic (enabled by default)
4. Retention: **7 days** (Starter), **30 days** (Standard+)

#### Manual Extended Retention:
For Starter plan users needing 30-day retention:

```bash
# Create backup script: scripts/backup.sh
#!/bin/bash
set -e

BACKUP_DIR="./backups"
RETENTION_DAYS=30
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/backup_$TIMESTAMP.sql.gz"

# Create backup directory
mkdir -p $BACKUP_DIR

# Run pg_dump
pg_dump $DATABASE_URL | gzip > $BACKUP_FILE

echo "Backup created: $BACKUP_FILE"

# Delete backups older than retention period
find $BACKUP_DIR -name "backup_*.sql.gz" -mtime +$RETENTION_DAYS -delete

echo "Old backups cleaned up (kept last $RETENTION_DAYS days)"
```

#### Set Up Cron Job (via Render Cron Service):
1. Create new **Cron Job** service in Render
2. Set schedule: `0 2 * * *` (daily at 2 AM)
3. Command: `bash scripts/backup.sh`
4. Set environment variable: `DATABASE_URL` (from PostgreSQL service)

#### Restore:
```bash
# Download backup and restore
gunzip -c backup_20251206.sql.gz | psql $DATABASE_URL
```

**Cost:** 
- Starter: Free (7-day retention)
- Standard: $7/month (30-day retention)

---

### Option 3: Supabase

Supabase provides **automated daily backups** with configurable retention.

#### Setup Steps:
1. Project Dashboard â†’ Database â†’ Backups
2. **Point-in-Time Recovery (PITR)** enabled on Pro plan
3. Automatic daily backups included
4. Retention: **7 days** (Free), **30+ days** (Pro)

#### Manual Backup Script:
```bash
# Create backup using Supabase CLI
npx supabase db dump -f backup.sql

# Or via pg_dump
pg_dump "postgresql://postgres:[password]@db.[project-ref].supabase.co:5432/postgres" > backup.sql
```

#### Automated Backups (GitHub Actions):
Create `.github/workflows/db-backup.yml`:

```yaml
name: Database Backup

on:
  schedule:
    - cron: '0 2 * * *' # Daily at 2 AM UTC
  workflow_dispatch: # Manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      
      - name: Install PostgreSQL Client
        run: sudo apt-get install -y postgresql-client
      
      - name: Create Backup
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          pg_dump "$DATABASE_URL" | gzip > backup_$TIMESTAMP.sql.gz
      
      - name: Upload to Artifact
        uses: actions/upload-artifact@v3
        with:
          name: database-backup
          path: backup_*.sql.gz
          retention-days: 30
      
      - name: Upload to S3 (Optional)
        if: env.AWS_S3_BUCKET != ''
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
        run: |
          aws s3 cp backup_*.sql.gz s3://$AWS_S3_BUCKET/backups/
```

#### Restore:
```bash
# Restore from backup file
gunzip -c backup.sql.gz | psql $DATABASE_URL

# Or via Supabase CLI
npx supabase db reset --db-url $DATABASE_URL
```

**Cost:** 
- Free: 7-day retention
- Pro: $25/month (PITR + 30-day retention)

---

### Option 4: Heroku

Heroku provides **automatic daily backups** via PG Backups addon.

#### Setup Steps:
```bash
# Install PG Backups addon
heroku addons:create heroku-postgresql:standard-0

# Backups are automatic with Standard and above plans
# Retention: 7 days (Standard), 14 days (Premium), 30 days (Enterprise)

# Manual backup
heroku pg:backups:capture --app your-app-name

# List backups
heroku pg:backups --app your-app-name

# Download backup
heroku pg:backups:download --app your-app-name

# Restore backup
heroku pg:backups:restore b001 DATABASE_URL --app your-app-name
```

#### Automated Schedule:
```bash
# Schedule daily backups at 2 AM
heroku pg:backups:schedule DATABASE_URL --at '02:00 UTC' --app your-app-name

# Verify schedule
heroku pg:backups:schedules --app your-app-name
```

**Cost:**
- Hobby: No backups
- Standard-0: $50/month (7-day retention)
- Premium: $200/month (14-day retention)

---

### Option 5: Self-Hosted / Local PostgreSQL

For self-hosted PostgreSQL on VPS (DigitalOcean, AWS EC2, etc.)

#### Backup Script
Create `/opt/scripts/pg_backup.sh`:

```bash
#!/bin/bash
# PostgreSQL Automated Backup Script
# Run daily via cron

set -e

# Configuration
BACKUP_DIR="/var/backups/postgresql"
RETENTION_DAYS=30
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
DATABASE_NAME="your_database_name"
DATABASE_USER="postgres"

# Backup file
BACKUP_FILE="$BACKUP_DIR/${DATABASE_NAME}_${TIMESTAMP}.sql.gz"

# Create backup directory
mkdir -p $BACKUP_DIR

# Log file
LOG_FILE="$BACKUP_DIR/backup.log"

echo "[$(date)] Starting backup..." >> $LOG_FILE

# Perform backup
pg_dump -U $DATABASE_USER -d $DATABASE_NAME | gzip > $BACKUP_FILE

if [ $? -eq 0 ]; then
    echo "[$(date)] Backup completed: $BACKUP_FILE" >> $LOG_FILE
    
    # Get file size
    SIZE=$(du -h $BACKUP_FILE | cut -f1)
    echo "[$(date)] Backup size: $SIZE" >> $LOG_FILE
else
    echo "[$(date)] ERROR: Backup failed!" >> $LOG_FILE
    exit 1
fi

# Delete old backups
echo "[$(date)] Cleaning old backups (keeping last $RETENTION_DAYS days)..." >> $LOG_FILE
find $BACKUP_DIR -name "${DATABASE_NAME}_*.sql.gz" -mtime +$RETENTION_DAYS -delete

# Count remaining backups
BACKUP_COUNT=$(find $BACKUP_DIR -name "${DATABASE_NAME}_*.sql.gz" | wc -l)
echo "[$(date)] Total backups: $BACKUP_COUNT" >> $LOG_FILE

echo "[$(date)] Backup process completed successfully" >> $LOG_FILE
```

#### Make Script Executable:
```bash
chmod +x /opt/scripts/pg_backup.sh
```

#### Set Up Cron Job:
```bash
# Edit crontab
crontab -e

# Add this line (runs daily at 2 AM)
0 2 * * * /opt/scripts/pg_backup.sh

# Verify cron job
crontab -l
```

#### Restore Script
Create `/opt/scripts/pg_restore.sh`:

```bash
#!/bin/bash
set -e

if [ -z "$1" ]; then
    echo "Usage: $0 <backup_file.sql.gz>"
    exit 1
fi

BACKUP_FILE="$1"
DATABASE_NAME="your_database_name"
DATABASE_USER="postgres"

echo "Restoring from: $BACKUP_FILE"
echo "WARNING: This will overwrite the current database!"
read -p "Are you sure? (yes/no): " CONFIRM

if [ "$CONFIRM" != "yes" ]; then
    echo "Restore cancelled"
    exit 0
fi

# Drop and recreate database
echo "Dropping database..."
psql -U $DATABASE_USER -c "DROP DATABASE IF EXISTS $DATABASE_NAME;"
psql -U $DATABASE_USER -c "CREATE DATABASE $DATABASE_NAME;"

# Restore from backup
echo "Restoring backup..."
gunzip -c $BACKUP_FILE | psql -U $DATABASE_USER -d $DATABASE_NAME

echo "Restore completed successfully!"
```

#### Usage:
```bash
# Create backup manually
sudo /opt/scripts/pg_backup.sh

# Restore from backup
sudo /opt/scripts/pg_restore.sh /var/backups/postgresql/mydb_20251206.sql.gz
```

---

## ðŸ“¦ Advanced: Cloud Storage Integration

### Upload to AWS S3

Add to your backup script:

```bash
# Install AWS CLI
sudo apt-get install awscli -y

# Configure credentials
aws configure set aws_access_key_id YOUR_ACCESS_KEY
aws configure set aws_secret_access_key YOUR_SECRET_KEY
aws configure set default.region us-east-1

# Upload to S3
S3_BUCKET="my-db-backups"
aws s3 cp $BACKUP_FILE s3://$S3_BUCKET/backups/$(basename $BACKUP_FILE)

# Set lifecycle policy for 30-day retention
aws s3api put-bucket-lifecycle-configuration --bucket $S3_BUCKET --lifecycle-configuration '{
  "Rules": [{
    "Id": "DeleteOldBackups",
    "Status": "Enabled",
    "Prefix": "backups/",
    "Expiration": { "Days": 30 }
  }]
}'
```

### Upload to Google Cloud Storage

```bash
# Install gcloud CLI
curl https://sdk.cloud.google.com | bash

# Authenticate
gcloud auth login

# Upload to GCS
gsutil cp $BACKUP_FILE gs://my-db-backups/backups/

# Set lifecycle policy
gsutil lifecycle set lifecycle.json gs://my-db-backups

# lifecycle.json:
{
  "lifecycle": {
    "rule": [{
      "action": {"type": "Delete"},
      "condition": {"age": 30}
    }]
  }
}
```

---

## ðŸ§ª Testing Your Backups

**CRITICAL:** Always test your backup restoration process!

### Test Procedure:

```bash
# 1. Create a test backup
pg_dump $DATABASE_URL > test_backup.sql

# 2. Create a test database
psql $DATABASE_URL -c "CREATE DATABASE test_restore;"

# 3. Restore to test database
psql postgresql://user:pass@host:5432/test_restore < test_backup.sql

# 4. Verify data
psql postgresql://user:pass@host:5432/test_restore -c "SELECT COUNT(*) FROM escrows;"

# 5. Clean up
psql $DATABASE_URL -c "DROP DATABASE test_restore;"
```

### Automated Test Script:

Create `scripts/test_backup.sh`:

```bash
#!/bin/bash
set -e

BACKUP_FILE="$1"
TEST_DB="test_restore_$(date +%s)"

echo "Testing backup: $BACKUP_FILE"

# Create test database
psql $DATABASE_URL -c "CREATE DATABASE $TEST_DB;"

# Restore
gunzip -c $BACKUP_FILE | psql "${DATABASE_URL%/*}/$TEST_DB"

# Verify tables exist
TABLE_COUNT=$(psql "${DATABASE_URL%/*}/$TEST_DB" -t -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema='public';")

if [ "$TABLE_COUNT" -gt 0 ]; then
    echo "âœ… Backup test PASSED - $TABLE_COUNT tables restored"
else
    echo "âŒ Backup test FAILED - No tables found"
    exit 1
fi

# Clean up
psql $DATABASE_URL -c "DROP DATABASE $TEST_DB;"
```

---

## ðŸ“Š Monitoring Backup Health

### Backup Monitoring Checklist:

- [ ] Verify backups run daily
- [ ] Check backup file sizes are reasonable
- [ ] Test restore process monthly
- [ ] Monitor backup storage usage
- [ ] Set up alerts for backup failures

### Slack Notifications:

Add to your backup script:

```bash
# Slack webhook URL
SLACK_WEBHOOK="https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

# Send notification
curl -X POST -H 'Content-type: application/json' --data "{
  \"text\": \"âœ… Database backup completed successfully\n*File:* $BACKUP_FILE\n*Size:* $SIZE\n*Time:* $(date)\"
}" $SLACK_WEBHOOK
```

### Email Notifications:

```bash
# Install mailutils
sudo apt-get install mailutils -y

# Send email
echo "Database backup completed: $BACKUP_FILE (Size: $SIZE)" | mail -s "DB Backup Success" admin@yourdomain.com
```

---

## ðŸ”’ Security Best Practices

### 1. Encrypt Backups
```bash
# Encrypt with GPG
pg_dump $DATABASE_URL | gzip | gpg --encrypt --recipient admin@yourdomain.com > backup.sql.gz.gpg

# Decrypt
gpg --decrypt backup.sql.gz.gpg | gunzip | psql $DATABASE_URL
```

### 2. Restrict Backup Access
```bash
# Set proper permissions
chmod 600 /var/backups/postgresql/*.sql.gz
chown postgres:postgres /var/backups/postgresql/*.sql.gz
```

### 3. Use Read-Only Database User
```sql
-- Create read-only backup user
CREATE USER backup_user WITH PASSWORD 'secure_password';
GRANT CONNECT ON DATABASE your_db TO backup_user;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO backup_user;

-- Use in backup script
pg_dump -U backup_user -d your_db > backup.sql
```

---

## ðŸ“ Backup Checklist

### Daily (Automated):
- [x] Full database backup
- [x] Compress backup files
- [x] Upload to cloud storage
- [x] Verify backup size
- [x] Clean old backups (>30 days)

### Weekly (Manual):
- [ ] Test backup restoration
- [ ] Verify backup integrity
- [ ] Check storage usage
- [ ] Review backup logs

### Monthly (Manual):
- [ ] Full restoration test
- [ ] Update backup scripts
- [ ] Review retention policy
- [ ] Audit backup security

---

## ðŸ†˜ Recovery Procedures

### Scenario 1: Partial Data Loss
```bash
# Restore specific table from backup
pg_restore -U postgres -d your_db -t escrows backup.dump
```

### Scenario 2: Complete Database Loss
```bash
# 1. Download latest backup
# 2. Recreate database
psql -U postgres -c "CREATE DATABASE your_db;"

# 3. Restore
gunzip -c latest_backup.sql.gz | psql -U postgres -d your_db

# 4. Verify restoration
psql -U postgres -d your_db -c "\dt"
```

### Scenario 3: Point-in-Time Recovery (PITR)
Requires WAL archiving (advanced):
```bash
# Enable WAL archiving in postgresql.conf
wal_level = replica
archive_mode = on
archive_command = 'cp %p /path/to/archive/%f'

# Restore to specific timestamp
pg_restore --create --clean --if-exists backup.dump
```

---

## ðŸ’° Cost Comparison

| Platform | Backup Frequency | Retention | Cost |
|----------|-----------------|-----------|------|
| Railway | Daily (automatic) | 7 days | Free |
| Render Starter | Daily (automatic) | 7 days | Free |
| Render Standard | Daily (automatic) | 30 days | $7/mo |
| Supabase Free | Daily (automatic) | 7 days | Free |
| Supabase Pro | PITR + Daily | 30+ days | $25/mo |
| Heroku Standard | Daily (automatic) | 7 days | $50/mo |
| Self-Hosted + S3 | Daily (custom) | 30 days | ~$1-5/mo |

**Recommendation:** Railway or Render Standard for best value

---

## âœ… Implementation Complete

Your database backup system is now configured for:
- âœ… Automated daily backups
- âœ… 30-day retention policy
- âœ… Tested restoration procedures
- âœ… Monitoring and alerts
- âœ… Secure encrypted backups

**Next Steps:**
1. Choose your platform from options above
2. Set up automated backups
3. Test restoration process
4. Schedule monthly backup tests
5. Document recovery procedures for your team

---

*Last Updated: December 6, 2025*
*Status: Production Ready âœ…*
